{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import math\n",
    "import operator\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "f_train_pos = open('IMDb/train/imdb_train_pos.txt','r', encoding=\"UTF-8\")\n",
    "f_train_neg = open('IMDb/train/imdb_train_neg.txt','r', encoding=\"UTF-8\")\n",
    "\n",
    "f_test_pos = open('IMDb/test/imdb_test_pos.txt','r', encoding=\"UTF-8\")\n",
    "f_test_neg = open('IMDb/test/imdb_test_neg.txt','r', encoding=\"UTF-8\")\n",
    "\n",
    "f_dev_pos = open('IMDb/dev/imdb_dev_pos.txt','r', encoding=\"UTF-8\")\n",
    "f_dev_neg = open('IMDb/dev/imdb_dev_neg.txt','r', encoding=\"UTF-8\")\n",
    "\n",
    "p_in = open('positive.txt','r')\n",
    "n_in = open('negative.txt','r')\n",
    "\n",
    "pos_words = [x.strip(\"\\n\") for x in p_in]\n",
    "neg_words = [x.strip(\"\\n\") for x in n_in]\n",
    "\n",
    "train_pos = []\n",
    "train_neg = []\n",
    "test_pos = []\n",
    "test_neg = []\n",
    "dev_pos = []\n",
    "dev_neg = []\n",
    "\n",
    "for line in f_train_pos:\n",
    "    train_pos.append(line)\n",
    "for line in f_train_neg:\n",
    "    train_neg.append(line)\n",
    "for line in f_test_pos:\n",
    "    test_pos.append(line)\n",
    "for line in f_test_neg:\n",
    "    test_neg.append(line)\n",
    "for line in f_dev_pos:\n",
    "    dev_pos.append(line)\n",
    "for line in f_dev_neg:\n",
    "    dev_neg.append(line)\n",
    "    \n",
    "\n",
    "train_pos = preprocess_reviews(train_pos)\n",
    "train_neg = preprocess_reviews(train_neg)\n",
    "test_pos = preprocess_reviews(test_pos)\n",
    "test_neg = preprocess_reviews(test_neg)\n",
    "dev_pos = preprocess_reviews(dev_pos)\n",
    "dev_neg = preprocess_reviews(dev_neg)\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "dev_set = []\n",
    "\n",
    "train_set += [(x,1) for x in train_pos]\n",
    "train_set += [(x,0) for x in train_neg]\n",
    "test_set += [(x,1) for x in test_pos]\n",
    "test_set += [(x,0) for x in test_neg]\n",
    "dev_set += [(x,1) for x in dev_pos]\n",
    "dev_set += [(x,0) for x in dev_neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_tokens(string):\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "            \n",
    "    return list_tokens\n",
    "\n",
    "def get_lemmatizer():\n",
    "    return nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "    # We can add more words to the stopword list, like punctuation marks\n",
    "    #stopwords.add(\".\")\n",
    "    #stopwords.add(\",\")\n",
    "    #stopwords.add(\"#\")\n",
    "    #stopwords.add(\"@\")\n",
    "    #stopwords.add(\":\")\n",
    "    #stopwords.add(\"--\")\n",
    "    #stopwords.add(\"``\")\n",
    "    #stopwords.add(\"!\")\n",
    "    #stopwords.add(\"?\")\n",
    "    #stopwords.add(\"...\")\n",
    "    #stopwords.add(\"&\")\n",
    "    #stopwords.add(\"-\")\n",
    "    #stopwords.add(\";\")\n",
    "    #stopwords.add(\"'\")\n",
    "    #stopwords.add(\"#\")\n",
    "    #stopwords.add(\"â€™\")\n",
    "    return stopwords\n",
    "    \n",
    "def get_vocabulary(training_set, num_features): # Function to retrieve vocabulary\n",
    "    dict_word_frequency={}\n",
    "    for instance in training_set:\n",
    "        sentence_tokens=get_list_tokens(instance[0])\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "            else: dict_word_frequency[word]+=1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "    return vocabulary\n",
    "\n",
    "def get_vector_text(list_vocab, string):\n",
    "    vector_text=np.zeros(len(list_vocab))\n",
    "    list_tokens_string=get_list_tokens(string)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n",
    "\n",
    "def get_vector_text_pos_neg(list_vocab, string):\n",
    "    vector_text=np.zeros(len(list_vocab)+2)\n",
    "    list_tokens_string=get_list_tokens(string)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    vector_text[i+1] = pos_word_count(list_tokens_string)\n",
    "    vector_text[i+2] = neg_word_count(list_tokens_string)\n",
    "    return vector_text\n",
    "\n",
    "def get_vector_text_all(list_vocab, string):\n",
    "    vector_text=np.zeros(len(list_vocab)+4)\n",
    "    list_tokens_string=get_list_tokens(string)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    p_scores = vader.polarity_scores(list_to_sentance(list_tokens_string))\n",
    "    vector_text[i+1] = p_scores['neg']\n",
    "    vector_text[i+2] = p_scores['neu']\n",
    "    vector_text[i+3] = p_scores['pos']\n",
    "    vector_text[i+4] = p_scores['compound']\n",
    "    return vector_text\n",
    "\n",
    "def list_to_sentance(list_string):\n",
    "    str_rtn = \"\"\n",
    "    for word in list_string:\n",
    "        str_rtn += word + \" \"\n",
    "    return str_rtn\n",
    "\n",
    "\n",
    "def pos_word_count(tokens):\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in pos_words:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def neg_word_count(tokens):\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in neg_words:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = get_lemmatizer()\n",
    "stopwords = get_stopwords()\n",
    "vocabulary = get_vocabulary(train_set, 2000)\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "Y_train = [x[1] for x in train_set]\n",
    "Y_test = [x[1] for x in test_set]\n",
    "Y_dev = [x[1] for x in dev_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector count and VADER analysis\n",
    "Xvec = [(get_vector_text_all(vocabulary, x[0]), x[1]) for x in train_set]\n",
    "Xvec_test = [(get_vector_text_all(vocabulary, x[0]), x[1]) for x in test_set]\n",
    "Xvec_dev = [(get_vector_text_all(vocabulary, x[0]), x[1]) for x in dev_set]\n",
    "\n",
    "# TF-IDF vectorisation\n",
    "tfidf_vec = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=True, max_features=500)\n",
    "tfX = tfidf_vec.fit_transform(train_pos+train_neg)\n",
    "tfX_test = tfidf_vec.transform(test_pos+test_neg)\n",
    "tfX_dev = tfidf_vec.transform(dev_pos+dev_neg)\n",
    "\n",
    "# combining features\n",
    "tfX_reshape = scipy.sparse.csr_matrix.toarray(tfX)\n",
    "tfX_test_reshape = scipy.sparse.csr_matrix.toarray(tfX_test)\n",
    "tfX_dev_reshape = scipy.sparse.csr_matrix.toarray(tfX_dev)\n",
    "\n",
    "Xvec_all = Xvec.copy()\n",
    "Xvec_all_std = Xvec.copy()\n",
    "Xvec_all_test = Xvec_test.copy()\n",
    "Xvec_all_test_std = Xvec_test.copy()\n",
    "Xvec_all_dev = Xvec_dev.copy()\n",
    "Xvec_all_dev_std = Xvec_dev.copy()\n",
    "\n",
    "for i in range(0, len(tfX_reshape)):\n",
    "    Xvec_all[i] = (np.append(Xvec_all[i][0], np.asarray(tfX_reshape[i])), Xvec_all[i][1])\n",
    "for i in range(0, len(tfX_test_reshape)):\n",
    "    Xvec_all_test[i] = np.append(Xvec_all_test[i][0], np.asarray(tfX_test_reshape[i]))\n",
    "for i in range(0, len(tfX_dev_reshape)):\n",
    "    Xvec_all_dev[i] = np.append(Xvec_all_dev[i][0], np.asarray(tfX_dev_reshape[i]))\n",
    "    \n",
    "scaler = sklearn.preprocessing.StandardScaler()    \n",
    "nx_all = [x[0] for x in Xvec_all]\n",
    "std_x = scaler.fit_transform(nx_all)\n",
    "std_x_test = scaler.transform(Xvec_all_test)\n",
    "std_x_dev = scaler.transform(Xvec_all_dev)\n",
    "\n",
    "pca_transformer = sklearn.decomposition.PCA(n_components=20)\n",
    "pca_x = pca_transformer.fit_transform(std_x)\n",
    "pca_x_test = pca_transformer.transform(std_x_test)\n",
    "pca_x_dev = pca_transformer.transform(std_x_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_clf = train_svm_vector_classifier(std_x)\n",
    "svm_clf = sklearn.svm.SVC(kernel='rbf', gamma='scale', C=0.8)\n",
    "t_start_std = time.perf_counter()\n",
    "svm_clf.fit(std_x, Y_train)\n",
    "t_std = time.perf_counter() - t_start_std\n",
    "\n",
    "svm_clf_pca = sklearn.svm.SVC(kernel='rbf', gamma='scale', C=0.8)\n",
    "t_start_pca = time.perf_counter()\n",
    "svm_clf_pca.fit(pca_x, Y_train)\n",
    "t_pca = time.perf_counter() - t_start_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "preds = svm_clf.predict(std_x_test)\n",
    "t_std_pred = time.perf_counter() - t\n",
    "\n",
    "t = time.perf_counter()\n",
    "preds_pca = svm_clf_pca.predict(pca_x_test)\n",
    "t_pca_pred = time.perf_counter() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87      2501\n",
      "           1       0.86      0.91      0.88      2499\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      2501\n",
      "           1       0.85      0.89      0.87      2499\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "\n",
      "Without PCA learn t= 509.2410146000002    predict t= 99.98504760000105\n",
      "With PCA learn t= 4.074947699999029    predict t= 0.7835511999983282\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(Y_test, preds))\n",
    "print()\n",
    "print(sklearn.metrics.classification_report(Y_test, preds_pca))\n",
    "print()\n",
    "print(\"Without PCA learn t=\",t_std,\"   predict t=\",t_std_pred)\n",
    "print(\"With PCA learn t=\",t_pca,\"   predict t=\",t_pca_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev set parameter optimisation\n",
    "\n",
    "- PCA components (show time vs accuracy)\n",
    "- SVM C parameter\n",
    "- Possibly vocab feature size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = [5,10,20,50,100,500,1000]\n",
    "acc_list_comp = []\n",
    "t_learn_list_comp = []\n",
    "t_pred_list_comp = []\n",
    "\n",
    "for n in comp_list:\n",
    "\n",
    "    pca_transformer = sklearn.decomposition.PCA(n_components=n)\n",
    "    pca_x = pca_transformer.fit_transform(std_x)\n",
    "    #pca_x_test = pca_transformer.transform(std_x_test)\n",
    "    pca_x_dev = pca_transformer.transform(std_x_dev)\n",
    "\n",
    "    svm_clf_pca = sklearn.svm.SVC(kernel='rbf', gamma='scale')\n",
    "    t_start_pca = time.perf_counter()\n",
    "    svm_clf_pca.fit(pca_x, Y_train)\n",
    "    t_pca = time.perf_counter() - t_start_pca\n",
    "\n",
    "    t = time.perf_counter()\n",
    "    preds_pca = svm_clf_pca.predict(pca_x_dev)\n",
    "    t_pca_pred = time.perf_counter() - t\n",
    "    \n",
    "    acc_list_comp.append(sklearn.metrics.accuracy_score(Y_dev, preds_pca))\n",
    "    t_learn_list_comp.append(t_pca)\n",
    "    t_pred_list_comp.append(t_pca_pred)\n",
    "    \n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM C regularisation parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = [2.0,1.5,1.0,0.8,0.6,0.4,0.2,0.1]\n",
    "acc_list_c = []\n",
    "t_learn_list_c = []\n",
    "\n",
    "pca_transformer = sklearn.decomposition.PCA(n_components=20)\n",
    "pca_x = pca_transformer.fit_transform(std_x)\n",
    "pca_x_dev = pca_transformer.transform(std_x_dev)\n",
    "\n",
    "for c in c_list:\n",
    "    svm_clf_pca = sklearn.svm.SVC(kernel='rbf', gamma='scale', C=c)\n",
    "    t_start_pca = time.perf_counter()\n",
    "    svm_clf_pca.fit(pca_x, Y_train)\n",
    "    t_pca = time.perf_counter() - t_start_pca\n",
    "\n",
    "    t = time.perf_counter()\n",
    "    preds_pca = svm_clf_pca.predict(pca_x_dev)\n",
    "    t_pca_pred = time.perf_counter() - t\n",
    "    \n",
    "    acc_list_c.append(sklearn.metrics.accuracy_score(Y_dev, preds_pca))\n",
    "    t_learn_list_c.append(t_pca)\n",
    "    \n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [100,200,500,1000,2000]\n",
    "acc_list_voc = []\n",
    "t_vectorize_list_voc = []\n",
    "t_learn_list_voc = []\n",
    "\n",
    "for vf in vocab_list:\n",
    "    t = time.perf_counter()\n",
    "    vocabulary = get_vocabulary(train_set, vf)\n",
    "    # vector count and VADER analysis\n",
    "    Xvec = [(get_vector_text_all(vocabulary, x[0]), x[1]) for x in train_set]\n",
    "    Xvec_dev = [(get_vector_text_all(vocabulary, x[0]), x[1]) for x in dev_set]\n",
    "\n",
    "    # TF-IDF vectorisation\n",
    "    tfidf_vec = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=True, max_features=500)\n",
    "    tfX = tfidf_vec.fit_transform(train_pos+train_neg)\n",
    "    tfX_dev = tfidf_vec.transform(dev_pos+dev_neg)\n",
    "\n",
    "    # combining features\n",
    "    tfX_reshape = scipy.sparse.csr_matrix.toarray(tfX)\n",
    "    tfX_dev_reshape = scipy.sparse.csr_matrix.toarray(tfX_dev)\n",
    "\n",
    "    Xvec_all = Xvec.copy()\n",
    "    Xvec_all_std = Xvec.copy()\n",
    "    Xvec_all_dev = Xvec_dev.copy()\n",
    "    Xvec_all_dev_std = Xvec_dev.copy()\n",
    "\n",
    "    for i in range(0, len(tfX_reshape)):\n",
    "        Xvec_all[i] = (np.append(Xvec_all[i][0], np.asarray(tfX_reshape[i])), Xvec_all[i][1])\n",
    "    for i in range(0, len(tfX_dev_reshape)):\n",
    "        Xvec_all_dev[i] = np.append(Xvec_all_dev[i][0], np.asarray(tfX_dev_reshape[i]))\n",
    "    \n",
    "    scaler = sklearn.preprocessing.StandardScaler()    \n",
    "    nx_all = [x[0] for x in Xvec_all]\n",
    "    std_x = scaler.fit_transform(nx_all)\n",
    "    std_x_dev = scaler.transform(Xvec_all_dev)\n",
    "\n",
    "    pca_transformer = sklearn.decomposition.PCA(n_components=20)\n",
    "    pca_x = pca_transformer.fit_transform(std_x)\n",
    "    pca_x_dev = pca_transformer.transform(std_x_dev)\n",
    "    \n",
    "    t_vec = time.perf_counter() - t\n",
    "    \n",
    "    svm_clf_pca = sklearn.svm.SVC(kernel='rbf', gamma='scale', C=0.8)\n",
    "    t = time.perf_counter()\n",
    "    svm_clf_pca.fit(pca_x, Y_train)\n",
    "    t_pca = time.perf_counter() - t\n",
    "\n",
    "    t = time.perf_counter()\n",
    "    preds_pca = svm_clf_pca.predict(pca_x_dev)\n",
    "    t_pca_pred = time.perf_counter() - t\n",
    "    \n",
    "    acc_list_voc.append(sklearn.metrics.accuracy_score(Y_dev, preds_pca))\n",
    "    t_learn_list_voc.append(t_pca)\n",
    "    t_vectorize_list_voc.append(t_vec)\n",
    "    \n",
    "    print(vf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(l, str_file):\n",
    "    f = open(str_file, 'w')\n",
    "    f.writelines([str(i) + \"\\n\" for i in l])\n",
    "    \n",
    "write_file(comp_list, 'Dev_results/comp_list.txt')\n",
    "write_file(c_list, 'Dev_results/c_list.txt')\n",
    "write_file(vocab_list, 'Dev_results/vocab_list.txt')\n",
    "\n",
    "write_file(acc_list_comp, 'Dev_results/acc_list_comp.txt')\n",
    "write_file(acc_list_c, 'Dev_results/acc_list_c.txt')\n",
    "write_file(acc_list_voc, 'Dev_results/acc_list_voc.txt')\n",
    "\n",
    "write_file(t_learn_list_comp, 'Dev_results/t_learn_list_comp.txt')\n",
    "write_file(t_learn_list_c, 'Dev_results/t_learn_list_c.txt')\n",
    "write_file(t_learn_list_voc, 'Dev_results/t_learn_list_voc.txt')\n",
    "write_file(t_vectorize_list_voc, 'Dev_results/t_vectorize_list_voc.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
